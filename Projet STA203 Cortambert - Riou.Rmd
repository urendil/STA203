---
title: "Projet STA203 Cortambert - Riou"
author: "Pierre Cortambret - William Riou"
date: "28/04/2021"
output: pdf_document
---
# Introduction

Le but de cette analyse est de créer un modèle qui permet de déterminer la teneur en sucre des cookies par spectrométrie. Le spectre s'étend sur 700 fréquences (variables explicatives) sur 72 individus (cookies). En travaillant sur ce jeu de données, les méthodes de statistique inférentielle rencontrent rapidement les limites des jeux de données de taille importante. Il va falloir déterminer quelques variables explicatives dont le $cos2$ est important pour pouvoir modéliser plus facilement le problème.

# 1 - Un peu de théorie
```{r}
rm(list=objects())
graphics.off()
setwd("~/OneDrive/Cours/2020-2021/08-STA203/PROJET")
```

## Question 1
La régression Ridge est particulièrement adaptée lorsque nous travaillons en grande dimension (où le nombre de variables explicatives sont très nombreuses). Le cas échéant, les variables sont souvent fortement corrélées. Le conditionnement de la matrice de corrélation est alors très mauvais (les $\lambda_i$ tendent vers $0$). Afin de compenser ce mauvais conditionnement, nous pénalisons les variables.

## Question 2
Nous cherchon à calculer l'estimateur des coefficients $\hat{\theta}$.
L'estimateur des coefficients est défini soit par un problème d'optimisation sous contraintes:

$$\hat{\theta} = argmin_{\theta \in \mathbb{R}^p;||\theta||_1 \leq \delta} ||Y-X \theta||^2$$

soit comme une fonction des paramètres de pénalisation $\kappa$:

$$\hat{\theta}_{ridge}(\kappa) = (X^{'}X+\kappa Id_p)^{-1}*X^{'}Y$$


Créons le jeu d'apprentissage
```{r}
load("cookie.app.RData")
xtrain <- data.frame(cookie.app)
n <- nrow(xtrain)
p <- ncol(xtrain)
ytrain <- xtrain[,1]
xtrain <- xtrain[,-1]
dim(xtrain)
```

Créons maintenant le jeu de test:
```{r}
load("cookie.val.RData")
xtest <- data.frame(cookie.val)
n1 <- nrow(xtest)
p1 <- ncol(xtest)
ytest <- xtest[,1]
xtest <- xtest[,-1]
dim(xtest)
```

```{r}
sigma2 <- 1
eps <- rnorm(700,mean=0,sd=sigma2)
x <- xtrain + eps
res.lm <- lm(ytrain~.,data = x)
s <- summary(res.lm)
```


# 2 - Analyse exploratoire

## 1 - Mise en forme des données

```{r}
par(mfrow=c(1,2))
plot(ytrain,main="Jeu d'apprentissage",xlab = "Nombre d'individus")
abline(h=mean(ytrain),lty=2,col="red")
plot(ytest,main="Jeu de test",xlab="Nombre d'individus")
abline(h=mean(ytrain),lty=2,col="blue")
```
Nous explorons les deux jeux de données (apprentissage et test). Nous n'observons aucune tendance dans la teneur en sucre dans les cookies.


## 2 - Analyse par composantes principales
```{r}
boxplot(xtrain,main="Représentation en boxplot \n du jeu d'apprentissage",xlab="Fréquences",ylim=c(0,2))

matplot(t(xtrain),main="Représentation du \n jeu d'apprentissage",xlab="Fréquences",ylab = "xtrain",ylim=c(0,2),type="l")

boxplot(t(xtrain),main="Représentation en boxplot \n du jeu d'apprentissage",xlab="Cookies",ylim=c(0,2))

matplot(xtrain,main="Représentation du \n jeu d'apprentissage",xlab="Cookies",ylab = "xtrain",ylim=c(0,2),type="l")

```
Nous remarquons une tendance générale sur les 40 individus. Nous remarquons aussi que l'écart-type des variables augmentent avec la fréquence.

Une étude n'est pas faisable avec 700 variables explicatives. Il faut procéder préalablement à une analyse par composantes principales. 

Ne retenons que quelques variables de façon aléatoire par les 700 variables.
Créons donc un échantillon avec les variables $X1$,$X10$,$X50$,$X100$,$X250$,$X500$,$X600$ et $X700$.
```{r}
sample<-c(1,10,50,100,250,500,600,700)
x.sample<-apply(xtrain[,sample],2,sd)
plot(x.sample,main="écart-type des échantillons X1, X10, X50, X100, X500 et X700",ylab="variance",type="o",col="red")
```

```{r}
library(corrplot)
X <- scale(xtrain[,sample], scale = TRUE, center = TRUE)
C <- cor(X)
corrplot(C)
```
Nous constatons que la corrélation des variables entre elles diminuent avec la fréquence. La corrélation entre $X1$ et $X50$ est plus importante qu'entre les variables $X1$ et $X700$.

## 2 - Analyse par composantes principales
```{r message=TRUE, warning=FALSE}
library(FactoMineR)
library(factoextra)
pca.res <- PCA(xtrain,graph = FALSE, scale.unit = TRUE)
fviz_eig(pca.res,main = "Histogramme des valeurs propres de l'analyse par composantes principales",addlabels = TRUE)
```
Refaisons maintenant l'analyse préliminaire avec les 6 premières dimensions.

Sélectionnons les individus 1, 2, 3, 4, 5 et 39.

```{r}
y <- c(1:5,39)
x <- scale(xtrain[y,], scale = TRUE , center = TRUE)
C.sample <- cor(x)
corrplot(C.sample)
```



```{r}
par(mfrow=c(3,2))
fviz_pca_biplot(pca.res,axes = c(1,2),label="ind",col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),select.var = list(cos2 = 0.99))
fviz_pca_biplot(pca.res,axes = c(2,3),label="ind",col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),select.var = list(cos2 = 0.5))
fviz_pca_biplot(pca.res,axes = c(3,4),label="ind",col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),select.var = list(cos2 = 0.1))
fviz_pca_biplot(pca.res,axes = c(4,5),label="ind",col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),select.var = list(cos2 = 0.01))
fviz_pca_biplot(pca.res,axes = c(5,1),label="ind",col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),select.var = list(cos2 = 0.9))
```

## 3 - Codage de la fonction reconstruct

Nous construisons la fonction reconstruct qui construit dans le nuage des variables dans les $nr$ premières dimensions. Nous ajoutons sur cette représentation le vecteur moyenne $Xm$ et le vecteur des écarts-type $Xsd$.

```{r}
Xm <- apply(xtrain , 1 , mean)
Xsd <- apply(xtrain , 1 , sd)

reconstruct <- function(res,nr,Xm,Xsd){
  par(mfrow=c(abs(nr/2)+1,2))
  for (i in 1:nr){
    plot(res,axes=i:i+1,choix = "var",col.var="cos2")
    arrows(0,0,Xsd[i],Xsd[i+1],col = "blue")
    arrows(0,0,Xm[i],Xm[i+1],col = "blue")
  }
}
```


Reconstruisons le nuage des variables dans les 6 premières dimensions.

```{r}
reconstruct(pca.res,1,Xm,Xsd)
```



```{r}
library(caret)
par(mfrow=c(2,3))
y <- c(1:5,39)

#plot(xtrain[y,],main = paste("RMSE = ",round(c(RMSE(xtrain[y,],ytrain[y])),2),"\n MAE = ",round(c(MAE(xtrain[y,],ytrain[y]))),4))
  
indi = 24
plot(xtrain[y,indi],main = paste("RMSE = ",round(c(RMSE(xtrain[y,indi],ytrain)),2),"\n MAE = ",round(c(MAE(xtrain[y,indi],ytrain))),4),ylab="X24",xlab="Individus")
abline(h=mean(xtrain[y,indi]),col="red",lty=2)
```

# 3 - Régression pénalisée

## 1 - 

### lambda de 10^6 à 10^-10
```{r}
library(glmnet)
gridRidge <- 10^seq(6,-10,length=100)
```
$\lambda=0$ : pas de pénalisation Ridge, revient à minimiser Somme des Carrés Résiduels.
$\lambda=+\infty$ : pénalisation Ridge maximale.

```{r}
x <- cbind(ytrain,xtrain)

xRidge_train <- model.matrix(ytrain~., data=x)
ridge_fit <- glmnet(xRidge_train, ytrain,alpha = 0, lambda=gridRidge)
plot(ridge_fit$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept",type="o")

abline(h=ridge_fit$a0[which.max(ridge_fit$a0)],col="red",lty=2)
abline(v=which.max((ridge_fit$a0)),col="red",lty=2)
ridge_fit$a0[which.max(ridge_fit$a0)]
```

```{r}
coef.ridge=coef(ridge_fit)[-1,]
dim(coef.ridge)

plot(ridge_fit$lambda[y],xlim=c(0,200))
abline(h=0,col="red")
```


```{r}
theta_chap <- function(k){ (t(xtrain)*xtrain+k*diag(1,nrow=p))^(-1)*t(xtrain)*ytrain
}
k <- seq(0,10,by = 1)
plot(unlist(theta_chap(k)),main="Régression Ridge",ylab="theta_chap",xlab="kappa",type="o",xlim=c(0,10),col="blue")
```

## Centrage de Y et de X

```{r}
y.scale <- scale(ytrain, scale = TRUE , center = TRUE )
x.scale <- as.data.frame(cbind(y.scale,scale(xtrain , scale=TRUE , center=TRUE) ))
xRidge_train.scale <- model.matrix(y.scale~., data=x.scale)
ridge_fit.scale <- glmnet(xRidge_train.scale, y.scale,alpha = 0, lambda=gridRidge)
plot(ridge_fit.scale$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept")
abline(v = which.max(ridge_fit.scale$a0),col="red",lty=2)
abline(h = ridge_fit.scale$a0[which.max(ridge_fit.scale$a0)],col="red",lty=2)
ridge_fit$a0[which.max(ridge_fit.scale$a0)]
```


## Centrage de X et non de Y
```{r}
x.scale <- as.data.frame(cbind(ytrain,scale(xtrain , scale=TRUE , center=TRUE) ))
xRidge_train.scale <- model.matrix(ytrain~., data=x.scale)
ridge_fit.scale <- glmnet(xRidge_train.scale, ytrain,alpha = 0, lambda=gridRidge)
plot(ridge_fit.scale$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept")

abline(v = which.max(ridge_fit.scale$a0),col="red",lty=2)
abline(h = ridge_fit.scale$a0[which.max(ridge_fit.scale$a0)],col="red",lty=2)
ridge_fit$a0[which.max(ridge_fit.scale$a0)]
```

## Centrage de Y et non de X
```{r}
y.scale <- scale(ytrain, scale = TRUE , center = TRUE )
x.scale <- as.data.frame(cbind(y.scale,xtrain))
xRidge_train.scale <- model.matrix(y.scale~., data=x.scale)
ridge_fit.scale <- glmnet(xRidge_train.scale, y.scale,alpha = 0, lambda=gridRidge)
plot(ridge_fit.scale$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept")

abline(v = which.max(ridge_fit.scale$a0),col="red",lty=2)
abline(h = ridge_fit.scale$a0[which.max(ridge_fit.scale$a0)],col="red",lty=2)
ridge_fit.scale$a0[which.max(ridge_fit.scale$a0)]
```


# 4 - Régression logistique pénalisée

## 1 -
L'expérience est modélisée comme la réalisation de $n$ variables aléatoires indépendantes $Y_i$ de loi de Bernoulli d'espérance $\pi(x_i)=\mathbb{P}(Y_i=1;x_i):\\$
$Y_i \rightarrow \mathcal{B}(1,\pi(x_i))$

telle que $logit(\pi(x_i))=log(\frac{\pi(x_i)}{1-\pi(x_i)})=x_i\theta$
La variable aléatoire $Y$ suit une loi binomiale $\mathcal{B}(1,\pi(x_k))$

```{r}
z <- ifelse(ytrain>18,1,0)
ztest <- ifelse(ytest>18,1,0)
sum(z)
sum(ztest)
```

```{r}
par(mfrow=c(1,2))
plot(z,main = paste("Jeu d'apprentissage \n moyenne = ",mean(z)))
abline(h=mean(z),col="red",lty=2)

plot(ztest,main = paste("Jeu de validation \n moyenne = ",round(mean(ztest),3)))
abline(h=mean(ztest),col="blue",lty=2)
```
Le jeu d'apprentissage n'est pas équilibré. La moyenne est égale à 0,4.
Un jeu de données équilibré aurait eu une moyenne de 0,5.

Le jeu de validation n'est pas équilibré non plus. Cependant, les deux jeux (apprentissage/validation) ont un équilibre (moyenne) équivalente.



## 2 - 

```{r}
library(pls)
set.seed(1)
cvseg <- cvsegments(nrow(xtrain),k=4,type="random")
```



```{r}
library(MASS)
par(mfrow=c(1,2))
plot(lm.ridge(ytrain~.,data = xtrain,lambda=seq(0,1,0.01)))
plot(lm.ridge(ytrain~.,data = xtrain,lambda=seq(0,0.2,0.001)))
```


```{r}
plot(lm.ridge(ytrain~.,data = xtrain,lambda=gridRidge))
```

```{r}
choix.kappa <- function(kappamax,cvseg,nbe=100){
  press=rep(0,nbe)
  for (i in 1:length(cvseg)){
    cook.app <- cbind(ytrain,xtrain)
    valid <- cook.app[unlist(cvseg[i]),]
    
    modele <- lm.ridge(ytrain~.,data=cook.app[unlist(cvseg[-i]),],lambda = seq(0,kappamax,length=nbe))
    coeff <- coef(modele)
    prediction <- matrix(coeff[,1],nrow(coeff),nrow(valid))+coeff[,-1]%*%t(data.matrix(valid[,-1]))
    press <- press+rowSums((matrix(valid[,1],nrow(coeff),nrow(valid),byrow=T)-prediction)**2)
  }
  kappaet <- seq(0,kappamax,length=nbe)[which.min(press)]
  return(list(kappaet = kappaet,press=press))
}
```


```{r}
res <- choix.kappa(0.5,cvseg,nb=1000)
plot(seq(0,0.5,length = 1000), res$press,main = "PRESS",xlab="nombre d'échantillons",ylab="Predicted Sum of Squares")
```
```{r}
kappaet <- res$kappaet
x.ridge <- lm.ridge(ytrain~.,data = xtrain,lambda = kappaet)
coeff<-coef(x.ridge)
```


```{r}
plot.res=function(x,y,titre="titre")
{
plot(x,y,col="blue",ylab="Résidus",
xlab="Valeurs prédites",main=titre)
abline(h=0,col="green")
}
```


```{r}
par(mfrow=c(1,2))
cook.app <- cbind(ytrain,xtrain)
fit.rid <- rep(coeff[1],nrow(cook.app))+as.vector(coeff[-1]%*%t(data.matrix(cook.app[,-1])))
plot(fit.rid,cook.app[,1],xlab="Valeurs prédites",ylab = "ytrain")
res.rid <- fit.rid-cook.app[,1]
plot.res(fit.rid,res.rid,titre="Résidus")
```

```{r}
cook.test <- cbind(ytest,xtest)
ychap <- rep(coeff[1],nrow(cook.test))+as.vector(coeff[-1]%*%t(data.matrix(cook.test[,-1])))
plot(ychap,cook.test[,1],xlab = "Valeurs prédites",ylab="ytrain")
mean((cook.test[,1]-ychap**2))
```

