---
title: "Projet STA203 Cortambert - Riou"
author: "Pierre Cortambert - William Riou"
date: "28/04/2021"
output: pdf_document
---
# Introduction

Le but de cette analyse est de créer un modèle qui permet de déterminer la teneur en sucre des cookies par spectrométrie. Le spectre s'étend sur 700 fréquences (variables explicatives) sur 72 individus (cookies). En travaillant sur ce jeu de données, les méthodes de statistique inférentielle rencontrent rapidement les limites des jeux de données de taille importante. Il va falloir déterminer quelques variables explicatives pour pouvoir modéliser plus facilement le problème.

# 1 - Un peu de théorie
```{r}
rm(list=objects())
graphics.off()
setwd("~/OneDrive/Cours/2020-2021/08-STA203/PROJET")
```

## Question 1
La régression Ridge est particulièrement adaptée lorsque nous travaillons en grande dimension (où le nombre de variables explicatives sont très nombreuses). Le cas échéant, les variables sont souvent fortement corrélées. Le conditionnement de la matrice de corrélation est alors très mauvais (les $\lambda_i$ tendent vers $0$). Afin de compenser ce mauvais conditionnement, nous pénalisons les variables.

## Question 2
Nous cherchon à calculer l'estimateur des coefficients $\hat{\theta}$.
L'estimateur des coefficients est défini soit par un problème d'optimisation sous contraintes:

$$\hat{\theta} = argmin_{\theta \in \mathbb{R}^p;||\theta||_1 \leq \delta} ||Y-X \theta||^2$$

soit comme une fonction des paramètres de pénalisation $\kappa$:

$$\hat{\theta}_{ridge}(\kappa) = (X^{'}X+\kappa Id_p)^{-1}*X^{'}Y$$


Créons le jeu d'apprentissage
```{r}
load("cookie.app.RData")
xtrain <- data.frame(cookie.app)
n <- nrow(xtrain)
p <- ncol(xtrain)
ytrain <- xtrain[,1]
xtrain <- xtrain[,-1]
dim(xtrain)
```

Créons maintenant le jeu de test:
```{r}
load("cookie.val.RData")
xtest <- data.frame(cookie.val)
n1 <- nrow(xtest)
p1 <- ncol(xtest)
ytest <- xtest[,1]
xtest <- xtest[,-1]
dim(xtest)
```
Et finalement rajoutons un bruit gaussien à notre modèle.


```{r}
set.seed(1)
sigma2 <- 0.1
eps <- rnorm(700,mean=0,sd=sigma2)
x.train <- xtrain + eps
res.lm <- lm(ytrain~.,data = x.train)
s <- summary(res.lm)
```

Mais comment choisir les paramètres du bruit gaussien?

```{r}
par(mfrow=c(3,2))
for (i in c(1,0.7,0.5,0.2,0.1,0.01))
{
  eps <- rnorm(700,mean=0,sd=sigma2)
  x.train <- xtrain + eps
  plot(seq(1,700),x.train[1,],main=c("Données bruitées - sigma =",i),type="l",xlab="Fréquences",ylab="X1")
}
```
Afin que notre modèle ne soit pas trop bruité et que nos données ne soient pas "noyées" dans le bruit pour que ce soit réaliste, nous choisissons une valeur de $\sigma$ de 0.1.


# 2 - Analyse exploratoire

## 1 - Mise en forme des données

```{r}
par(mfrow=c(1,2))
plot(ytrain,main="Jeu d'apprentissage",xlab = "Nombre d'individus")
abline(h=mean(ytrain),lty=2,col="red")
plot(ytest,main="Jeu de test",xlab="Nombre d'individus")
abline(h=mean(ytrain),lty=2,col="blue")
```




Nous explorons les deux jeux de données (apprentissage et test). Nous n'observons aucune tendance dans la teneur en sucre dans les cookies. Les données présentent une grande variance à peu près équivalente dans les jeux apprentissage et test.


    
```{r}
boxplot(x.train,main="Représentation en boxplot \n du jeu d'apprentissage",xlab="Fréquences",ylim=c(0,2))

matplot(t(x.train),main="Représentation du \n jeu d'apprentissage",xlab="Fréquences",ylab = "xtrain",ylim=c(0,2),type="l")

boxplot(t(x.train),main="Représentation en boxplot \n du jeu d'apprentissage",xlab="Cookies",ylim=c(0,2))

matplot(x.train,main="Représentation du \n jeu d'apprentissage",xlab="Cookies",ylab = "xtrain",ylim=c(0,2),type="l")

```
Nous remarquons une tendance générale sur les 40 individus.

Une étude n'est pas faisable avec 700 variables explicatives. Il faut procéder préalablement à une analyse par composantes principales. 

Ne retenons que quelques variables de façon aléatoire par les 700 variables.
Créons donc un échantillon avec les variables $X1$,$X10$,$X50$,$X100$,$X250$,$X500$,$X600$ et $X700$.
```{r}
par(mfrow=c(1,2))
sample<-c(1,10,50,100,250,500,600,700)
x.sample<-apply(x.train[,sample],2,sd)
x.mean<-apply(x.train[,sample],2,mean)
plot(x.sample,
     main="écart-type des échantillons\n X1, X10, X50, X100, X500 et X700",
     ylab="écart-type",type="o",col="red")
plot(x.mean,
     main="moyenne des échantillons\n X1, X10, X50, X100, X500 et X700",
     ylab="moyenne",type="o",col="blue")
```
Le graphique nous montre effectivement une augmentation de la moyenne avec la fréquence. L'écart-type ne semble pas suivre de tendance.

```{r}
library(corrplot)
X <- scale(x.train[,sample], scale = TRUE, center = TRUE)
C <- cor(X)
corrplot(C)
```
Nous pouvons suppposer dès maintenant que l'analyse exploratoire va être compliquée considérant la corrélation des variables. Tentons quand même une analyse par composantes principales pour confirmer la forte corrélation des variables.

## 2 - Analyse par composantes principales
```{r message=TRUE, warning=FALSE}
library(FactoMineR)
library(factoextra)
pca.res <- PCA(x.train,graph = FALSE, scale.unit = TRUE)
fviz_eig(pca.res,
         main = "Histogramme des valeurs propres de l'analyse par composantes principales",addlabels = TRUE)
```
```{r}
vectp <- eigen(cor(x.train))$vectors
valp <- eigen(cor(x.train))$values
which(get_eig(pca.res)[,3]<70)
barplot(get_eig(pca.res)[1:9,2],main="Représentation de 70% de l'information")
```
Nous gardons les 9 premières dimensions dont les valeurs propres rassemblent 70% de l'information.
```{r}
x.pca <- (x.train*vectp)[,1:9]
```

L'ACP ne permet pas de réduire les dimensions en suivant la règle du "coude" sauf si nous voulons garder qu'un axe et perdre 67% de l'information.
Les valeurs propres des variables suivantes tendent vers 0. Si nous voulons faire une analyse plus poussée, il faudra considérer de pénaliser les paramètres (Ridge ou LASSO).

Réalisons maintenant une ACP sur les 5 premiers axes principaux en faisant apparaître le $cos2$. Voici les graphiques obtenus:

```{r}
par(mfrow=c(3,2))
fviz_pca_biplot(pca.res,axes = c(1,2),label="ind",
                col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),
                select.var = list(cos2 = 0.5))
fviz_pca_biplot(pca.res,axes = c(2,3),label="ind",
                col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),
                select.var = list(cos2 = 0.3))
fviz_pca_biplot(pca.res,axes = c(3,4),label="ind",
                col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),
                select.var = list(cos2 = 0.1))
fviz_pca_biplot(pca.res,axes = c(4,5),label="ind",
                col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),
                select.var = list(cos2 = 0.01))
# fviz_pca_biplot(pca.res,axes = c(5,1),label="ind",
#               col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),
#               select.var = list(cos2 = 0.1))
fviz_pca_biplot(pca.res,axes = c(5,1),label="ind",
                col.var = "cos2",select.ind = list(name=c("1","2","3","4","5","39")),
                select.var = list(cos2 = 0.1))
```

Selectionnons les individus 1,2,3,4,5 et 39.

```{r}
y <- c(1:5,39)
x <- scale(x.pca[y,], scale = TRUE , center = TRUE)
C.sample <- cor(x)
corrplot(C.sample)
```

## 3 - Codage de la fonction reconstruct

Nous construisons la fonction reconstruct qui construit dans le nuage des variables dans les $nr$ premières dimensions. Nous ajoutons sur cette représentation le vecteur moyenne $Xm$ et le vecteur des écarts-type $Xsd$.

```{r}
Xm <- apply(x.pca , 1 , mean)
Xsd <- apply(x.pca , 1 , sd)

reconstruct <- function(res,nr,Xm,Xsd){
  par(mfrow=c(abs(nr/2)+1,2))
  for (i in 1:nr){
    plot(res,axes=i:i+1,choix = "var",col.var="cos2")
    arrows(0,0,Xsd[i],Xsd[i+1],col = "blue")
    arrows(0,0,Xm[i],Xm[i+1],col = "blue")
  }
}
```


Reconstruisons le nuage des variables dans les 6 premières dimensions.

```{r}
#reconstruct(pca.res,1,Xm,Xsd)
```



```{r message=FALSE, warning=FALSE}
library(caret)
par(mfrow=c(2,3))
for (i in y){
  glmModel <- glm(ytrain[i]~.,data = x.pca[i,])
  Preds <- predict(glmModel,type="response")
  plot(seq(1:9),x.pca[i,],
       main = paste("RMSE=",round(RMSE(ytrain[i],Preds),2),"\n MAE=",round(MAE(ytrain[i],Preds),4)),
       type = "l",
       xlab = "Fréquences",ylab=paste("Cookie ",i," max = ",round(max(x.pca[i,]),3)))
  abline(v = which.max(x.pca[i,]),col="red",lty=2)
  abline(h = max(x.pca[i,]),col="red",lty=2)
}
```
Il est difficile de comparer les courbes entre elles car la forme générale n'est pas identique. Prenons un critère (le maximum) afin de comparer ces courbes. Nous constatons que le maximum des courbes est différent:


Voici les maxima des différents graphiques en fonction des individus.
```{r}
round(apply(x.pca[y,],1,max),3)
```
De même, nous pouvons comparer les fréquences pour lesquelles ce maximum est atteint.
```{r}
round(apply(x.pca[y,],1,which.max),3)
```
Réappliquons cette méthode sur la variable $X24$ afin de déterminer l'erreur des moindres-carrés (RMSE) et l'erreur moyenne (MAE).

```{r}
par(mfrow=c(1,1))
indice <- 24
glmModel.indice <- glm(ytrain[indice]~.,data=x.train[indice,])
Preds <- predict(glmModel.indice,type="response")
plot(seq(1:9),x.pca[indice,],main = paste("RMSE=",round(RMSE(ytrain[indice],Preds),2),"\n MAE=",round(MAE(ytrain[indice],Preds),4)),type = "l",xlab = "Fréquences",ylab=paste("Cookie ",indice))
abline(v = which.max(x.pca[indice,]),col="red",lty=2)
abline(h = max(x.pca[indice,]),col="red",lty=2)
text(5,max(x.pca[indice,])-0.1,round(max(x.pca[indice,]),3),col="red")
```

Procédons maintenant à la recherche d'un modèle ajusté à partir du résultat de la PCA.
Nous allons définir un modèle en cherchant à minimiser l'AIC de chaque modèle en partant du modèle complet.

```{r}
library(MASS)
glmModel <- glm(ytrain~.,data = x.pca)
stepAIC(glmModel,direction = "both")
```

```{r}
stepAIC(glmModel)$anova
```


```{r}
plot(stepAIC(glmModel)$anova[,6],main="Evolution de l'AIC en fonction des itérations",xlab="Nombre d'itérations",ylab="AIC",type="l")
```

En comparant le modèle complet sur le modèle $X3$+$X5$, nous constatons 

Nous retenons finalement le modèle $X3$+$X5$
```{r}
res.lm2 <- glm(ytrain~X3+X5,data = x.pca)
anova(res.lm2,res.lm)
```


# 3 - Régression pénalisée

Nous avons vu dans les premières parties lors de l'analyse exploratoire des données (ACP notamment) que les valeurs propres tendaient rapidement vers $0$. Pour contrecarrer ce phénomène, nous avions l'idée de procéder à une pénalisation extérieure (Ridge ou LASSO) du problème lagrangien (min SCR) vu $\textit{supra}$. Cette pénalisation va nous permettre de déterminer des coefficients $\lambda$. Ces coefficients pourront ensuite nous aider à trouver un modèle ajusté avec le nombre suffisant de paramètres (AIC) avec une erreur de généralisation minimale.


## 1 - 

Commençons par créer un vecteur de $\lambda$ de $10^{-10}$ à $10^6$.

### lambda de 10^6 à 10^-10
```{r}
library(glmnet)
gridRidge <- 10^seq(6,-10,length=100)
```
$\lambda=0$ : pas de pénalisation Ridge, revient à minimiser des Somme des Carrés Résiduels.

$\lambda=+\infty$ : pénalisation Ridge maximale.

Créons un modèle issue d'une régression linéaire généralisée (pénalisée) puis déterminons la valeur à partir de laquelle $\lambda$ plonge vers $0$.
Tout d'abord, reprenons le jeu de données d'apprentissage initial que l'on nomme $x$.
```{r}
x <- cbind(ytrain,x.pca)
xRidge_train <- model.matrix(ytrain~., data=x.pca)
ridge_fit <- glmnet(xRidge_train, ytrain,alpha = 0, lambda=gridRidge)
plot(ridge_fit$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept",type="l")
abline(h=ridge_fit$a0[which.max(ridge_fit$a0)],col="red",lty=2)
abline(v=which.max((ridge_fit$a0)),col="red",lty=2)
abline(h=0)
ridge_fit$a0[which.max(ridge_fit$a0)]
text(which.max(ridge_fit$a0),-5,which.max(ridge_fit$a0),col="red")
```


```{r}
theta_chap <- function(k){ (t(x.pca)*x.pca+k*diag(1,nrow=nrow(x.pca)))^(-1)*t(x)*ytrain
}
k <- seq(1,100,by=0.1)

plot(unlist(theta_chap(k)),main="Régression Ridge",ylab="theta_chap",xlab="kappa",type="l",xlim=c(1,100),col="blue")

theta.ridge <- which.max(unlist(theta_chap(k)))
abline(v=which.max(unlist(theta_chap(k))),col="red",lty=2)
```
Nous en déduisons les valeurs ajustées de $Y_{ridge}$ avec un $\theta_{ridge}$=42 et $\hat{\sigma}^2=\frac{SCR(\hat{\theta_{ridge}})}{n-p}=\sum_i(Y_i-x_i\theta)^2$

```{r}
y.ridge <- function(kappa){
  sigma.chap <- apply((ytrain-x.pca*theta_chap(kappa))**2,1,sum)
  return(sigma.chap*(x.pca*theta_chap(kappa))+ytrain*diag(ytrain))
}
plot(y.ridge(k))
```


## Centrage de Y et de X

```{r}
y.scale <- scale(ytrain, scale = TRUE , center = TRUE )
x.scale <- as.data.frame(cbind(y.scale,scale(x.pca , scale=TRUE , center=TRUE) ))
xRidge_train.scale <- model.matrix(y.scale~., data=x.pca)
ridge_fit.scale <- glmnet(xRidge_train.scale, y.scale,alpha = 0, lambda=gridRidge)
plot(ridge_fit.scale$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept",type="l")
abline(v = 20,col="red",lty=2)
abline(h = ridge_fit.scale$a0[20],col="red",lty=2)
abline(h = 0)
ridge_fit$a0[20]
text(15,0.05,ridge_fit.scale$a0[20],col="red")
```
Nous obtenons une valeur du coefficient de l'intercept nulle.

## Centrage de X et non de Y
```{r}
x.scale <- as.data.frame(cbind(ytrain,scale(x.pca , scale=TRUE , center=TRUE) ))
xRidge_train.scale <- model.matrix(ytrain~., data=x.pca)
ridge_fit.scale <- glmnet(xRidge_train.scale, ytrain,alpha = 0, lambda=gridRidge)
plot(ridge_fit.scale$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept",type="l")

abline(v = which.max(ridge_fit.scale$a0),col="red",lty=2)
abline(h = ridge_fit.scale$a0[which.max(ridge_fit.scale$a0)],col="red",lty=2)
ridge_fit$a0[which.max(ridge_fit.scale$a0)]
```

## Centrage de Y et non de X
```{r}
y.scale <- scale(ytrain, scale = TRUE , center = TRUE )
x.scale <- as.data.frame(cbind(y.scale,x.pca))
xRidge_train.scale <- model.matrix(y.scale~., data=x.pca)
ridge_fit.scale <- glmnet(xRidge_train.scale, y.scale,alpha = 0, lambda=gridRidge)
plot(ridge_fit.scale$a0,main="évolution des coefficients de l'intercept",ylab="Valeur de l'intercept",type="l")

abline(v = which.max(ridge_fit.scale$a0),col="red",lty=2)
abline(h = ridge_fit.scale$a0[which.max(ridge_fit.scale$a0)],col="red",lty=2)
ridge_fit.scale$a0[which.max(ridge_fit.scale$a0)]
text(which.max(ridge_fit.scale$a0),4,which.max(ridge_fit.scale$a0),col="red")
```


# 4 - Régression logistique pénalisée

## 1 -
L'expérience est modélisée comme la réalisation de $n$ variables aléatoires indépendantes $Y_i$ de loi de Bernoulli d'espérance $\pi(x_i)=\mathbb{P}(Y_i=1;x_i):\\$
$Y_i \rightarrow \mathcal{B}(1,\pi(x_i))$

telle que $logit(\pi(x_i))=log(\frac{\pi(x_i)}{1-\pi(x_i)})=x_i\theta$
La variable aléatoire $Y$ suit une loi binomiale $\mathcal{B}(1,\pi(x_k))$

```{r}
z <- ifelse(ytrain>18,1,0)
ztest <- ifelse(ytest>18,1,0)
sum(z)
sum(ztest)
```

```{r}
par(mfrow=c(1,2))
plot(z,main = paste("Jeu d'apprentissage \n moyenne = ",mean(z)))
abline(h=mean(z),col="red",lty=2)

plot(ztest,main = paste("Jeu de validation \n moyenne = ",round(mean(ztest),3)))
abline(h=mean(ztest),col="blue",lty=2)
```
Le jeu d'apprentissage n'est pas équilibré. La moyenne est égale à 0,4.
Un jeu de données équilibré aurait eu une moyenne de 0,5.

Le jeu de validation n'est pas équilibré non plus. Cependant, les deux jeux (apprentissage/validation) ont un équilibre (moyenne) équivalente.



## 2 - 

```{r message=FALSE, warning=FALSE}
library(pls)
library(leaps)

B <- 5
folds <- cvsegments(ncol(x.pca),B, type="random")
```
Nous créons sur notre jeu de données un 3-fold pour une validation croisée et nous initialisons notre plan d'expérience.
Tentons d'estimer l'erreur de chaque modèle de la validation croisée afin de choisir le meilleur d'entre eux.

```{r}
lmtot = lm(ytrain~., data=x.pca)
X = model.matrix(lmtot)
cv.errors=matrix(NA,B,9, dimnames=list(NULL, paste0("Dim",1:9)))
cv.bestmod=matrix(NA,B,9, dimnames=list(NULL, paste0("Dim",1:9)))
```


Sur chaque échantillon x.pca[-subsetb], nous calculons pour chaque modèle:
-son meilleur ajustement;
-son erreur de prédiction sur le x.test[subsetb,];
-le nom des variables retenus dans Mk.


```{r}
# for(b in 1:B){
#   subsetb=unlist(folds[[b]])
#   best.fit=regsubsets(ytrain~.,data=x.pca[-subsetb,],nvmax=10)
#   # pour chaque taille de modèle
#   for(j in 1:9){
#     coefj=coef(best.fit,id=j)
#     pred=X[subsetb,names(coefj)]*coefj
#     cv.errors[b,j]=mean((ytrain[subsetb]-pred)**2)
#     cv.bestmod[b,j]=paste(names(coef(best.fit,id=j)[-1]),collapse="+")
#   }
# }
```




```{r}
library(MASS)
plot(lm.ridge(ytrain~.,data = x.pca,lambda=seq(0,100,0.1)),ylab="coefficients")
abline(h=0)
```

Créons maintenant une fonction qui nous permettra d'obtenir le $\kappa$ optimal qui minimise l'erreur généralisée du modèle.

```{r}
choix.kappa <- function(kappamax,cvseg,nbe=100){
  press=rep(0,nbe)
  for (i in 1:length(folds)){
    cook.app <- cbind(ytrain,x.pca)
    valid <- cook.app[unlist(folds[i]),]
    
    modele <- lm.ridge(ytrain~.,data=cook.app[unlist(folds[-i]),],lambda = seq(0,kappamax,length=nbe))
    coeff <- coef(modele)
    prediction <- matrix(coeff[,1],nrow(coeff),nrow(valid))+coeff[,-1]%*%t(data.matrix(valid[,-1]))
    press <- press+rowSums((matrix(valid[,1],nrow(coeff),nrow(valid),byrow=T)-prediction)**2)
  }
  kappaet <- seq(0,kappamax,length=nbe)[which.min(press)]
  return(list(kappaet = kappaet,press=press))
}
```

Analysons graphiquement le résultat:
```{r}
res <- choix.kappa(200,cvseg,nb=1000)
plot(seq(1,200,length = 1000), res$press,main = "PRESS",xlab="nombre d'échantillons",ylab="Predicted Sum of Squares",type="l")
```
Plus le $\kappa_{max}$ est élevé, plus l'erreur du modèle peut être réduite.

```{r}
kappaet <- res$kappaet
x.ridge <- lm.ridge(ytrain~.,data = x.pca,lambda = kappaet)
coeff <- coef(x.ridge)
```

Nous créons une fonction qui permet d'afficher les résidus.
```{r}
plot.res=function(x,y,titre="titre")
{
  plot(x,y,col="blue",ylab="Résidus",
  xlab="Valeurs prédites",main=titre)
  abline(h=0,col="green")
}
```

Nous allons maintenant comparer les valeurs réelles de la teneur en sucre ($ytrain$) avec la prédiction que nous faisons avec notre modèle de régression Ridge.
```{r}
par(mfrow=c(1,2))
cook.app <- cbind(ytrain,x.pca)
fit.rid <- rep(coeff[1],nrow(cook.app))+as.vector(coeff[-1]%*%t(data.matrix(cook.app[,-1])))
qqplot(fit.rid,cook.app[,1],main="Y_pred en fonction de ytrain",xlab="Valeurs prédites",ylab = "ytrain")
abline(a = 0 , b = 1,col="red")
res.rid <- fit.rid-cook.app[,1]
plot.res(fit.rid,res.rid,titre="Résidus")
```



```{r}
# cook.test <- cbind(ytest,xtest)
# ychap <- rep(coeff[1],nrow(cook.test))+as.vector(coeff[-1]%*%t(data.matrix(cook.test[,-1])))
# plot(ychap,cook.test[,1],xlab = "Valeurs prédites",ylab="ytrain")
# mean((cook.test[,1]-ychap**2))
# abline(a = 0 , b = 1 , col = "red")
```

