---
title: "Projet STA203 Cortambert - Riou"
author: "Pierre Cortambret - William Riou"
date: "28/04/2021"
output: pdf_document
---
# Introduction

Le but de cette analyse est de créer un modèle qui permet de déterminer la teneur en sucre des cookies par spectrométrie. Le spectre s'étend sur 700 fréquences (variables explicatives) sur 72 individus (cookies). En travaillant sur ce jeu de données, les méthodes de statistique inférentielle rencontrent rapidement les limites des jeux de données de taille importante. Il va falloir déterminer quelques variables explicatives dont le $cos2$ est important pour pouvoir modéliser plus facilement le problème.

# 1 - Un peu de théorie
```{r}
rm(list=objects())
graphics.off()
setwd("~/OneDrive/Cours/2020-2021/08-STA203/PROJET")
```

## Question 1
La régression Ridge est particulièrement adaptée lorsque nous travaillons en grande dimension (où le nombre de variables explicatives sont très nombreuses). Le cas échéant, les variables sont souvent fortement corrélées. Le conditionnement de la matrice de corrélation est alors très mauvais (les $\lambda_i$ tendent vers $0$). Afin de compenser ce mauvais conditionnement, nous pénalisons les variables.

## Question 2
Nous cherchon à calculer l'estimateur des coefficients $\hat{\theta}$.
L'estimateur des coefficients est défini soit par un problème d'optimisation sous contraintes:

$$\hat{\theta} = argmin_{\theta \in \mathbb{R}^p;||\theta||_1 \leq \delta} ||Y-X \theta||^2$$

soit comme une fonction des paramètres de pénalisation $\kappa$:

$$\hat{\theta}_{ridge}(\kappa) = (X^{'}X+\kappa Id_p)^{-1}*X^{'}Y$$


Créons le jeu d'apprentissage
```{r}
load("cookie.app.RData")
xtrain <- data.frame(cookie.app)
n <- nrow(xtrain)
p <- ncol(xtrain)
ytrain <- xtrain[,1]
xtrain <- xtrain[,-1]
dim(xtrain)
```

Créons maintenant le jeu de test:
```{r}
load("cookie.val.RData")
xtest <- data.frame(cookie.val)
n1 <- nrow(xtest)
p1 <- ncol(xtest)
ytest <- xtest[,1]
xtest <- xtest[,-1]
dim(xtest)
```

```{r}
sigma2 <- 1
eps <- rnorm(700,mean=0,sd=sigma2)
x <- xtrain + eps
res.lm <- lm(ytrain~.,data = x)
s <- summary(res.lm)
```


# 2 - Analyse exploratoire

## 1 - Mise en forme des données

```{r}
par(mfrow=c(1,2))
plot(ytrain,main="Jeu d'apprentissage",xlab = "Nombre d'individus")
abline(h=mean(ytrain),lty=2,col="red")
plot(ytest,main="Jeu de test",xlab="Nombre d'individus")
abline(h=mean(ytrain),lty=2,col="blue")
```



## 2 - Analyse par composantes principales
```{r}
boxplot(xtrain,main="Représentation en boxplot \n du jeu d'apprentissage",xlab="Fréquences",ylim=c(0,2))

matplot(t(xtrain),main="Représentation du \n jeu d'apprentissage",xlab="Fréquences",ylab = "xtrain",ylim=c(0,2),type="l")

boxplot(t(xtrain),main="Représentation en boxplot \n du jeu d'apprentissage",xlab="Cookies",ylim=c(0,2))

matplot(xtrain,main="Représentation du \n jeu d'apprentissage",xlab="Cookies",ylab = "xtrain",ylim=c(0,2),type="l")

```
Nous remarquons une tendance générale sur les 40 individus. Nous remarquons aussi que l'écart-type des variables augmentent avec la fréquence.

Une étude n'est pas faisable avec 700 variables explicatives. Il faut procéder préalablement à une analyse par composantes principales. 

Ne retenons que quelques variables de façon aléatoire par les 700 variables.
Créons donc un échantillon avec les variables $X1$,$X10$,$X50$,$X100$,$X250$,$X500$,$X600$ et $X700$.
```{r}
sample<-c(1,10,50,100,250,500,600,700)
x.sample<-apply(xtrain[,sample],2,sd)
plot(x.sample,main="écart-type des échantillons X1, X10, X50, X100, X500 et X700",ylab="variance")
```

```{r}
X <- scale(xtrain[,sample], scale = TRUE, center = TRUE)
library(corrplot)
C <- cor(X)
corrplot(C)
```
Nous constatons que la corrélation des variables entre elles diminuent avec la fréquence. La corrélation entre $X1$ et $X50$ est plus importante qu'entre les variables $X1$ et $X700$.

## 2 - Analyse par composantes principales
```{r message=TRUE, warning=FALSE}
library(FactoMineR)
library(factoextra)
pca.res <- PCA(xtrain,graph = FALSE, scale.unit = TRUE)
fviz_eig(pca.res,main = "Histogramme des valeurs propres de l'analyse par composantes principales",addlabels = TRUE)
```
Nous pouvons retenir 2 voire 3 dimensions.
Refaisons maintenant l'analyse préliminaire avec les 6 premières dimensions.
```{r message=FALSE}
lambda <- eigen(C,symmetric = TRUE)$values
Us <- eigen(C,symmetric = TRUE)$vectors
CP <- X%*%Us
CQ <- rbind(cor(X,CP)[1:5,],cor(X,CP)[39,])
```


Sélectionnons maintenant les individus 1, 2, 3, 4, 5 et 39.

```{r}
y <- c(1:5,39)
x <- scale(xtrain[y,], scale = TRUE , center = TRUE)
C.sample <- cor(x)
corrplot(C.sample)
```



```{r}
par(mfrow=c(3,2))
fviz_pca_var(pca.res,axes = c(1,2),repel = FALSE)
fviz_pca_var(pca.res,axes = c(2,3))
fviz_pca_var(pca.res,axes = c(3,4))
fviz_pca_var(pca.res,axes = c(4,5))
fviz_pca_var(pca.res,axes = c(5,1))
```

## 3 - Codage de la fonction reconstruct
```{r}
Xm <- apply(xtrain , 1 , mean)
Xsd <- apply(xtrain , 1 , sd)
reconstruct <- function(res,nr,Xm,Xsd){
  plot(PCA(res,axes = 1:nr),choix = "var")
  arrows(0,0,Xm%*%CQ[1],Xm%*%CQ[2])
}
#reconstruct(pca.res,6,Xm,Xsd)
```



```{r}
library(caret)
#par(mfrow=c(2,3))
y <- c(1,2,3,4,5,39)
indi = 24
plot(xtrain[y,indi],main = paste("RMSE = ",round(c(RMSE(xtrain[y,indi],ytrain)),2),"\n MAE = ",round(c(MAE(xtrain[y,indi],ytrain))),4),ylab="X24")
abline(h=mean(xtrain[y,indi]),col="red",lty=2)
```

# 3 - Régression pénalisée

## 1 - 

#lambda de 10^6 à 10^-10
```{r}
library(glmnet)
gridRidge <- 10^seq(6,-10,length=100)
```
lambda=0 : pas de Ridge regression penalty, revient à minimiser Somme des Carrés Résiduels
lambda=+inf : Ridge regression penalty max, séparation des données avec la moyenne de tous les variables explicatives
```{r}
x <- cbind(ytrain,xtrain)
xRidge_train <- model.matrix(ytrain~., data=x)
ridge_fit <- glmnet(xRidge_train, ytrain, alpha=0, lambda=gridRidge)
coef(ridge_fit)[1,]

coef.ridge=coef(ridge_fit)[-1,]
dim(coef.ridge)

plot(ridge_fit,xlim=c(0,20),ylim=c(-0.1,0.1))
abline(h=0,col="red")
```

# 4 - Régression logistique pénalisée

## 1 -
L'expérience est modélisée comme la réalisation de $n$ variables aléatoires indépendantes $Y_i$ de loi de Bernoulli d'espérance $\pi(x_i)=\mathbb{P}(Y_i=1;x_i):\\$
$Y_i \rightarrow \mathcal{B}(1,\pi(x_i))$

telle que $logit(\pi(x_i))=log(\frac{\pi(x_i)}{1-\pi(x_i)})=x_i\theta$
La variable aléatoire $Y$ suit une loi binomiale $\mathcal{B}(1,\pi(x_k))$

```{r}
z <- ifelse(ytrain>18,1,0)
ztest <- ifelse(ytest>18,1,0)
sum(z)
sum(ztest)
```

```{r}
par(mfrow=c(1,2))
plot(z,main = paste("Jeu d'apprentissage \n moyenne = ",mean(z)))
abline(h=mean(z),col="red",lty=2)

plot(ztest,main = paste("Jeu de validation \n moyenne = ",round(mean(ztest),3)))
abline(h=mean(ztest),col="blue",lty=2)
```
Le jeu d'apprentissage n'est pas équilibré. La moyenne est égale à 0,4.
Un jeu de données équilibré aurait eu une moyenne de 0,5.

Le jeu de validation n'est pas équilibré non plus. Cependant, les deux jeux (apprentissage/validation) ont un équilibre (moyenne) équivalente.



## 2 - 
```{r}
set.seed(1)
#cv.out=cv.glmnet(xtrain,z,nfolds = 10,lambda=gridRidge) 
```

